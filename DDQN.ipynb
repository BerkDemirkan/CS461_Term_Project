{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import gym\n",
    "import gymnasium as gym\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym_trading_env\n",
    "import gym_trading_env.environments\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ENV_WINDOWS = 10\n",
    "EVAL_PERIOD = 1\n",
    "RETURN_DIFFERENCES_TRAIN = []\n",
    "RETURN_DIFFERENCES_EVAL = []\n",
    "\n",
    "DEVICE = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "first = True\n",
    "\n",
    "import nvidia_smi\n",
    "nvidia_smi.nvmlInit()\n",
    "DEVICE_HANDLE = nvidia_smi.nvmlDeviceGetHandleByIndex(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Run only if loading\n",
    "dill.load_module(\"./ddqn_vars/ddqn_best.dill\")\n",
    "first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, state_dim):\n",
    "        self.max_size = max_size\n",
    "        self.mem_size = max_size\n",
    "\n",
    "        self.mem_ctr = 0\n",
    "        self.n = 0\n",
    "        self.calibrated = False\n",
    "\n",
    "        self.states = T.zeros(\n",
    "            (self.mem_size, state_dim), dtype=T.float32, device=DEVICE\n",
    "        )\n",
    "        self.actions = T.zeros(self.mem_size, dtype=T.int64, device=DEVICE)\n",
    "        self.next_states = T.zeros(\n",
    "            (self.mem_size, state_dim), dtype=T.float32, device=DEVICE\n",
    "        )\n",
    "        self.rewards = T.zeros(self.mem_size, dtype=T.float32, device=DEVICE)\n",
    "        self.dones = T.zeros(self.mem_size, dtype=T.bool, device=DEVICE)\n",
    "\n",
    "    def append(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        action: int,\n",
    "        next_states: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "    ):\n",
    "        idx = self.mem_ctr % self.mem_size\n",
    "\n",
    "        self.states[idx] = T.Tensor(\n",
    "            np.reshape(states, (np.multiply(*states.shape)))\n",
    "        ).to(DEVICE)\n",
    "        self.actions[idx] = action\n",
    "        self.next_states[idx] = T.Tensor(\n",
    "            np.reshape(next_states, (np.multiply(*next_states.shape)))\n",
    "        ).to(DEVICE)\n",
    "        self.rewards[idx] = reward\n",
    "        self.dones[idx] = done\n",
    "\n",
    "        self.mem_ctr += 1\n",
    "\n",
    "        if self.mem_ctr <= self.mem_size:\n",
    "            self.n = self.mem_ctr\n",
    "\n",
    "    def resize(self):\n",
    "        if not self.calibrated:\n",
    "            self.calibrated = True\n",
    "            self.mem_size = self.n\n",
    "\n",
    "            self.states = self.states[: self.mem_size]\n",
    "            self.actions = self.actions[: self.mem_size]\n",
    "            self.next_states = self.next_states[: self.mem_size]\n",
    "            self.rewards = self.rewards[: self.mem_size]\n",
    "            self.dones = self.dones[: self.mem_size]\n",
    "            print(f\"Resized buffer to {self.mem_size}\")\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        weights = T.ones(self.n, device=DEVICE).expand(batch_size, -1)\n",
    "        batch = T.multinomial(weights, 1, replacement=False).reshape(batch_size)\n",
    "\n",
    "        return (\n",
    "            self.states[batch],\n",
    "            self.actions[batch],\n",
    "            self.next_states[batch],\n",
    "            self.rewards[batch],\n",
    "            self.dones[batch],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, state_dims: int, fc1_dims: int, fc2_dims: int, action_dims: int, name: str\n",
    "    ):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        self._fc1 = nn.Linear(state_dims, fc1_dims)\n",
    "        self._fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self._fc3 = nn.Linear(fc2_dims, action_dims)\n",
    "\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, states: np.ndarray):\n",
    "        x = F.leaky_relu(self._fc1(states))\n",
    "        x = F.leaky_relu(self._fc2(x))\n",
    "        x = self._fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    class ThompsonSampling:\n",
    "        pass\n",
    "\n",
    "    class EpsilonGreedy:\n",
    "        pass\n",
    "\n",
    "    class Greedy:\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        fc1_dim=64,\n",
    "        fc2_dim=64,\n",
    "        batch_size=64,\n",
    "        max_mem_size=100_000,\n",
    "        learning_rate=1e-3,\n",
    "        discount_factor=0.99,\n",
    "        train_every=1,\n",
    "        sync_every=5_000,\n",
    "        tau=0.005,\n",
    "        epsilon=1,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.99,\n",
    "        decay_episodes: int = None,\n",
    "        Q1_name=\"QNetwork1\",\n",
    "        Q2_name=\"QNetwork2\",\n",
    "    ):\n",
    "        self._action_dim = action_dim\n",
    "        self._batch_size = batch_size\n",
    "        self._max_memory_size = max_mem_size\n",
    "        self._gamma = discount_factor\n",
    "        self._train_every = train_every\n",
    "        self._sync_every = sync_every\n",
    "        self._tau = tau\n",
    "        self._epsilon = epsilon\n",
    "        self._epsilon_end = epsilon_end\n",
    "        if decay_episodes and type(decay_episodes == int):\n",
    "            self._epsilon_decay = np.power(epsilon_end, 1 / decay_episodes)\n",
    "        else:\n",
    "            self._epsilon_decay = epsilon_decay\n",
    "\n",
    "        self._memory_size = 0\n",
    "        self._train_count = 0\n",
    "\n",
    "        self._QNetwork1 = DeepQNetwork(\n",
    "            state_dim, fc1_dim, fc2_dim, action_dim, name=Q1_name\n",
    "        )\n",
    "        self._QNetwork2 = DeepQNetwork(\n",
    "            state_dim, fc1_dim, fc2_dim, action_dim, name=Q2_name\n",
    "        )\n",
    "\n",
    "        self.update_networks(tau=1)\n",
    "        for param in self._QNetwork2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._optim = optim.Adam(self._QNetwork1.parameters(), lr=learning_rate)\n",
    "\n",
    "        self._memory = ReplayBuffer(max_mem_size, state_dim)\n",
    "\n",
    "    def choose_action(self, states: np.ndarray, algorithm=EpsilonGreedy):\n",
    "        states = T.Tensor(np.reshape(states, (np.multiply(*states.shape),))).to(DEVICE)\n",
    "        with T.no_grad():\n",
    "            values = self._QNetwork2(states)\n",
    "\n",
    "        match algorithm:\n",
    "            case DDQNAgent.Greedy:\n",
    "                action = np.argmax(values.cpu().numpy())\n",
    "            case DDQNAgent.EpsilonGreedy:\n",
    "                if np.random.random() < self._epsilon:  # epsilon greedy\n",
    "                    action = np.random.randint(0, self._action_dim)\n",
    "                else:\n",
    "                    action = np.argmax(values.cpu().numpy())\n",
    "            case DDQNAgent.ThompsonSampling:\n",
    "                action_probs = T.softmax(values, dim=0)\n",
    "                action = int(T.multinomial(action_probs, 1).cpu().detach())\n",
    "            case _:\n",
    "                raise NotImplementedError(\n",
    "                    f\"{algorithm} is not a valid action selection algorithm\"\n",
    "                )\n",
    "        return action\n",
    "\n",
    "    def remember(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        action: int,\n",
    "        next_states: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "    ):\n",
    "        self._memory.append(states, action, next_states, reward, done)\n",
    "\n",
    "    def resize_buffer(self):\n",
    "        self._memory.resize()\n",
    "\n",
    "    def train(self):\n",
    "        self._train_count += 1\n",
    "        if (self._memory.n <= self._batch_size) or (\n",
    "            self._train_count % self._train_every != 0\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        states, actions, next_states, rewards, dones = self._memory.sample(\n",
    "            self._batch_size\n",
    "        )\n",
    "\n",
    "        q_current_values = self._QNetwork1(states)\n",
    "\n",
    "        q_next_values = self._QNetwork1(next_states)\n",
    "        q_target_values = self._QNetwork2(next_states)\n",
    "\n",
    "        q_current = q_current_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        q_target = q_target_values.gather(\n",
    "            1, T.max(q_next_values, 1)[1].unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "\n",
    "        q_expected = rewards + self._gamma * q_target * (T.ones_like(dones) ^ dones)\n",
    "\n",
    "        loss = (q_current - q_expected.detach()).pow(2).mean()\n",
    "\n",
    "        self._optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optim.step()\n",
    "\n",
    "        self.update_networks()\n",
    "        # if self._train_count % self._sync_every == 0:\n",
    "        #     self.sync_networks()\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self._epsilon = max(self._epsilon_end, self._epsilon * self._epsilon_decay)\n",
    "\n",
    "    def update_networks(self, tau: float = None):\n",
    "        # self._QNetwork2.load_state_dict(self._QNetwork1.state_dict())\n",
    "\n",
    "        if tau == None:\n",
    "            tau = self._tau\n",
    "\n",
    "        target_value_params = self._QNetwork2.named_parameters()\n",
    "        value_params = self._QNetwork1.named_parameters()\n",
    "\n",
    "        target_value_state_dict = dict(target_value_params)\n",
    "        value_state_dict = dict(value_params)\n",
    "\n",
    "        for name in value_state_dict:\n",
    "            value_state_dict[name] = (\n",
    "                tau * value_state_dict[name].clone()\n",
    "                + (1 - tau) * target_value_state_dict[name].clone()\n",
    "            )\n",
    "\n",
    "        self._QNetwork2.load_state_dict(value_state_dict)\n",
    "\n",
    "    def save_models(self):\n",
    "        print(\"... Saving Models ...\")\n",
    "        T.save(self._QNetwork1.state_dict(), f\"DDQN_{self._QNetwork1.name}.model\")\n",
    "        T.save(self._QNetwork2.state_dict(), f\"DDQN_{self._QNetwork2.name}.model\")\n",
    "\n",
    "    def load_models(self):\n",
    "        print(\"... Loading Models ...\")\n",
    "        self._QNetwork1.load_state_dict(T.load(f\"DDQN_{self._QNetwork1.name}.model\"))\n",
    "        self._QNetwork2.load_state_dict(T.load(f\"DDQN_{self._QNetwork2.name}.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "    df[\"feature_open\"] = df[\"open\"] / df[\"close\"]\n",
    "    df[\"feature_high\"] = df[\"high\"] / df[\"close\"]\n",
    "    df[\"feature_low\"] = df[\"low\"] / df[\"close\"]\n",
    "    df[\"feature_volume\"] = df[\"volume\"] / df[\"volume\"].rolling(7 * 24).max()\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reward_function(history):\n",
    "    log_portfolio = np.log(\n",
    "        history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2]\n",
    "    )\n",
    "    log_market = np.log(history[\"data_close\", -1] / history[\"data_close\", -2])\n",
    "    reward = log_portfolio - log_market\n",
    "    return 1000 * reward\n",
    "\n",
    "\n",
    "def make_env(data_dir, positions, verbose=True):\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir=data_dir,\n",
    "        positions=positions,\n",
    "        preprocess=preprocess,\n",
    "        reward_function=reward_function,\n",
    "        windows=ENV_WINDOWS,\n",
    "        initial_position=0.0,\n",
    "        trading_fees=0.18 / 100,  # 0.18% per stock buy / sell (BTCTurk fees)\n",
    "        portfolio_initial_value=1_000,  # in USD\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    env.unwrapped.add_metric(\n",
    "        \"market_return_raw\",\n",
    "        lambda history: round(\n",
    "            100 * (history[\"data_close\", -1] / history[\"data_close\", 0] - 1), 2\n",
    "        ),\n",
    "    )\n",
    "    env.unwrapped.add_metric(\n",
    "        \"portfolio_return_raw\",\n",
    "        lambda history: round(\n",
    "            100\n",
    "            * (\n",
    "                history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", 0]\n",
    "                - 1\n",
    "            ),\n",
    "            2,\n",
    "        ),\n",
    "    )\n",
    "    env.unwrapped.add_metric(\n",
    "        \"position_changes\",\n",
    "        lambda history: 100\n",
    "        * np.sum(np.diff(history[\"position\"]) != 0)\n",
    "        / len(history[\"position\"]),\n",
    "    )\n",
    "    # env.unwrapped.add_metric(\"Episode Length\", lambda history: len(history))\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def plot_scores(\n",
    "    values: list,\n",
    "    y_axis_label: str,\n",
    "    label: str,\n",
    "    title: str,\n",
    "    file: str,\n",
    "    values2: list = None,\n",
    "    label2: str = None,\n",
    "    hlines: list = [0],\n",
    "    markers: list = [],\n",
    "):\n",
    "    x = np.arange(len(values)) + 1\n",
    "    if values2 != None:\n",
    "        y_max = max(max(values), max(values2), 0)\n",
    "        y_min = min(min(values), min(values2), 0)\n",
    "        x_eval = (np.arange(len(values2)) + 1) * EVAL_PERIOD\n",
    "    else:\n",
    "        y_max = max(max(values), 0)\n",
    "        y_min = min(min(values), 0)\n",
    "    y_ticks = np.arange(y_min, y_max, (y_max - y_min) / 11)\n",
    "\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(x, values, color=\"C0\", label=label)\n",
    "\n",
    "    if values2 != None:\n",
    "        plt.plot(x_eval, values2, color=\"C1\", label=label2)\n",
    "\n",
    "    for y in hlines:\n",
    "        plt.axhline(y=y, color=\"white\", lw=0.3)\n",
    "    for x, y, text in markers:\n",
    "        plt.text(x, y, text)\n",
    "\n",
    "    plt.yticks(y_ticks)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Environment and Agent Setup\n",
    "\n",
    "for file in glob.glob(\"./ddqn_vars/*.dill\"):\n",
    "    os.remove(file)\n",
    "for file in glob.glob(\"./ddqn_render/*.pkl\"):\n",
    "    os.remove(file)\n",
    "\n",
    "\n",
    "positions = list(np.arange(0, 1, 0.01))\n",
    "\n",
    "episode_ = 1\n",
    "\n",
    "training_env = make_env(\"./data/training/*.pkl\", positions, False)\n",
    "testing_env = make_env(\"./data/testing/*.pkl\", positions, False)\n",
    "\n",
    "agent = DDQNAgent(\n",
    "    np.multiply(*training_env.observation_space.shape),\n",
    "    training_env.action_space.n,\n",
    "    decay_episodes=int(EPISODES * 0.75),\n",
    "    max_mem_size=400_000,\n",
    "    tau=0.00001,\n",
    "    batch_size=256,\n",
    "    fc1_dim=256,\n",
    "    fc2_dim=256,\n",
    "    learning_rate=0.00005,\n",
    ")\n",
    "\n",
    "best_avg_score = -np.inf\n",
    "best_avg_score_eval = -np.inf\n",
    "scores = []\n",
    "scores_avg = []\n",
    "scores_eval = []\n",
    "scores_eval_avg = []\n",
    "epsilons = []\n",
    "times_taken = []\n",
    "best_episode_markers = []  # [(episode, score, text)]\n",
    "\n",
    "SAVE_MIN_LENGTH = 100\n",
    "BUFFER_EPISODES = np.inf\n",
    "SAVE_VARIABLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Main loop\n",
    "\n",
    "if first:\n",
    "    print(f\"Starting... Time: {datetime.now()}\\n\")\n",
    "else:\n",
    "    print(f\"Continuing... Time: {datetime.now()}\\n\")\n",
    "    n = len(scores)\n",
    "    for i in range(n):\n",
    "        print(\n",
    "            f\"Episode: {i+1:4d}, Score: {scores[i]:.3f}, \\\n",
    "              Average Score: {scores_avg[i]:.3f}, Epsilon: {epsilons[i]:.3f}\"\n",
    "        )\n",
    "\n",
    "calibrated = False\n",
    "for episode in tqdm(range(episode_, episode_ + EPISODES)):\n",
    "    if not calibrated:\n",
    "        gpu_memory_free = nvidia_smi.nvmlDeviceGetMemoryInfo(DEVICE_HANDLE).free / (1024**3)\n",
    "        if gpu_memory_free < 4:\n",
    "            print(f\"GPU memory limit reached at episode: {episode}.\")\n",
    "            agent.resize_buffer()\n",
    "            calibrated = True\n",
    "        elif episode > BUFFER_EPISODES:\n",
    "            print(f\"Requested buffer size reached.\")\n",
    "            agent.resize_buffer()\n",
    "            calibrated = True\n",
    "\n",
    "    time_start = time.perf_counter()\n",
    "\n",
    "    state, info = training_env.reset()\n",
    "\n",
    "    score, length, avg_length = 0, 0, 0\n",
    "    done, truncated = False, False\n",
    "    while not (done or truncated):\n",
    "        action = agent.choose_action(state, algorithm=DDQNAgent.ThompsonSampling)\n",
    "        next_state, reward, done, truncated, info = training_env.step(action)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        agent.remember(state, action, next_state, reward, done)\n",
    "        agent.train()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        length += 1\n",
    "    scores.append(score)\n",
    "    epsilons.append(agent._epsilon)\n",
    "    scores_avg.append(np.mean(scores[-SAVE_MIN_LENGTH:]))\n",
    "\n",
    "    metrics = training_env.unwrapped.results_metrics\n",
    "    market_return = metrics[\"market_return_raw\"]\n",
    "    portfolio_return = metrics[\"portfolio_return_raw\"]\n",
    "    position_changes = metrics[\"position_changes\"]\n",
    "    RETURN_DIFFERENCES_TRAIN.append(portfolio_return - market_return)\n",
    "\n",
    "    time_taken = time.perf_counter() - time_start\n",
    "    times_taken.append(time_taken)\n",
    "    print(\n",
    "        f\"Episode: {episode:>4}, Score: {score:>9.3f}, Market Return: {market_return:>7.2f}%, Portfolio Return: {portfolio_return:>8.2f}%, Position Changes: {position_changes:>6.2f}%, Time Taken: {time_taken:>5.2f}s, Length: {length:>4d}\"\n",
    "    )\n",
    "\n",
    "    if episode % EVAL_PERIOD == 0:\n",
    "        time_eval_start = time.perf_counter()\n",
    "        print(\"Evaluating...  \", end=\"\")\n",
    "\n",
    "        state_eval, info_eval = testing_env.reset()\n",
    "        score_eval = 0\n",
    "        done_eval, truncated_eval = False, False\n",
    "        while not (done_eval or truncated_eval):\n",
    "            action_ = agent.choose_action(state_eval, algorithm=DDQNAgent.Greedy)\n",
    "            (\n",
    "                state_eval,\n",
    "                reward_eval,\n",
    "                done_eval,\n",
    "                truncated_eval,\n",
    "                info_eval,\n",
    "            ) = testing_env.step(action_)\n",
    "            score_eval += reward_eval\n",
    "        scores_eval.append(score_eval)\n",
    "        scores_eval_avg.append(np.mean(scores_eval[-SAVE_MIN_LENGTH:]))\n",
    "\n",
    "        metrics_eval = testing_env.unwrapped.results_metrics\n",
    "        market_return_eval = metrics_eval[\"market_return_raw\"]\n",
    "        portfolio_return_eval = metrics_eval[\"portfolio_return_raw\"]\n",
    "        position_changes_eval = metrics_eval[\"position_changes\"]\n",
    "        RETURN_DIFFERENCES_EVAL.append(portfolio_return_eval - market_return_eval)\n",
    "\n",
    "        time_eval_taken = time.perf_counter() - time_eval_start\n",
    "        print(\n",
    "            f\"Score: {score_eval:>9.3f}, Market Return: {market_return_eval:>7.2f}%, Portfolio Return: {portfolio_return_eval:>8.2f}%, Position Changes: {position_changes_eval:>6.2f}%, Time Taken: {time_eval_taken:>5.2f}s, Free GPU Memory: {gpu_memory_free:>5.2f}GB\"\n",
    "        )\n",
    "\n",
    "        testing_env.unwrapped.save_for_render(dir=\"./ddqn_render\")\n",
    "        render_saves = glob.glob(\"./ddqn_render/*.pkl\")\n",
    "        latest_render_save = max(render_saves, key=os.path.getctime)\n",
    "        os.rename(\n",
    "            latest_render_save,\n",
    "            f\"./ddqn_render/ddqn_render_E{episode:04}_[{score_eval:+08.2f}].pkl\",\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            len(scores_eval_avg) >= SAVE_MIN_LENGTH\n",
    "            and scores_eval_avg[-1] > best_avg_score_eval\n",
    "        ):\n",
    "            best_avg_score_eval = scores_eval_avg[-1]\n",
    "            best_episode_markers.append(\n",
    "                (episode, score_eval, f\"{best_avg_score_eval:.1f}\")\n",
    "            )\n",
    "            print(\"Saving Variables...\")\n",
    "            shutil.copy(\n",
    "                f\"./ddqn_render/ddqn_render_E{episode:04}_[{score_eval:+08.2f}].pkl\",\n",
    "                f\"./ddqn_render/ddqn_best_E{episode:04}_[{best_avg_score_eval:+08.2f}].pkl\",\n",
    "            )\n",
    "            \n",
    "            if SAVE_VARIABLES:\n",
    "                try:\n",
    "                    dill.dump_module(\n",
    "                        f\"./ddqn_vars/ddqn_E{episode:04}_[{best_avg_score_eval:+08.2f}].dill\"\n",
    "                    )\n",
    "                    shutil.copy(\n",
    "                        f\"./ddqn_vars/ddqn_E{episode:04}_[{best_avg_score_eval:+08.2f}].dill\",\n",
    "                        f\"./ddqn_vars/ddqn_best.dill\",\n",
    "                    )\n",
    "                except:\n",
    "                    print(\"Failed to save variables. Disabling dill saves.\")\n",
    "                    SAVE_VARIABLES = False\n",
    "        \n",
    "        plot_scores(\n",
    "            values=scores,\n",
    "            values2=scores_eval,\n",
    "            y_axis_label=\"Score\",\n",
    "            label=\"Training Score\",\n",
    "            label2=\"Evaluation Score\",\n",
    "            title=\"Scores vs Episodes\",\n",
    "            file=\"DDQN_Scores.png\",\n",
    "        )\n",
    "        plot_scores(\n",
    "            values=RETURN_DIFFERENCES_TRAIN,\n",
    "            values2=RETURN_DIFFERENCES_EVAL,\n",
    "            y_axis_label=\"Return Difference (%)\",\n",
    "            label=\"Training Return Difference\",\n",
    "            label2=\"Evaluation Return Difference\",\n",
    "            title=\"Return Differences vs Episodes\",\n",
    "            file=\"DDQN_Returns.png\",\n",
    "        )\n",
    "        plot_scores(\n",
    "            values=times_taken,\n",
    "            y_axis_label=\"Training Time (s)\",\n",
    "            label=\"Training Time\",\n",
    "            title=\"Training Time vs Episodes\",\n",
    "            file=\"DDQN_Times.png\",\n",
    "            hlines=[],\n",
    "        )\n",
    "\n",
    "    agent.update_epsilon()\n",
    "    episode_ = episode + 1\n",
    "\n",
    "    total_time_taken = time.perf_counter() - time_start\n",
    "    print(f\"Total Time Taken: {total_time_taken}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from gym_trading_env.renderer import Renderer\n",
    "\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
